{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training With MLM and NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Train Dataset and Discretize into range [1,1000]\n",
    "# to make compatible with BERT\n",
    "\n",
    "df_train = pd.read_csv('data_train.csv', header=0)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = df_train.columns\n",
    "\n",
    "d_train = scaler.fit_transform(df_train)\n",
    "d_rounded_train = np.round(d_train*1000)\n",
    "df1_train = pd.DataFrame(d_rounded_train, columns=names)\n",
    "\n",
    "## reverse\n",
    "# d_scaled_train = df1_train/1000\n",
    "# d_reversed_train = scaler.inverse_transform(d_scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50/50 NSP Training Data\n",
    "\n",
    "df2_train = df1_train.sample(frac=0.5)\n",
    "df1_train.drop(df2_train.index.to_list(), inplace=True)\n",
    "\n",
    "df1_train['label'] = 0\n",
    "df2_train['label'] = 1\n",
    "\n",
    "df2_train[['discover_v', 'discover_d', 'discover_t']] = \\\n",
    "    df2_train[['discover_v', 'discover_d', 'discover_t']].sample(frac=1).values\n",
    "\n",
    "df1_train = df1_train.append(df2_train).sample(frac=1).reset_index(0, drop=True)\n",
    "\n",
    "df1_train.to_csv('output_train.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize TestDataset and Discretize into range [1,1000]\n",
    "# to make compatible with BERT\n",
    "\n",
    "df_test = pd.read_csv('data_test.csv', header=0)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = df_test.columns\n",
    "\n",
    "d_test = scaler.fit_transform(df_test)\n",
    "d_rounded_test = np.round(d_test*1000)\n",
    "df1_test = pd.DataFrame(d_rounded_test, columns=names)\n",
    "\n",
    "## reverse\n",
    "# d_scaled_test = df1_test/1000\n",
    "# d_reversed_test = scaler.inverse_transform(d_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50/50 NSP Test Data\n",
    "\n",
    "df2_test = df1_test.sample(frac=0.5)\n",
    "df1_test.drop(df2_test.index.to_list(), inplace=True)\n",
    "\n",
    "df1_test['label'] = 0\n",
    "df2_test['label'] = 1\n",
    "\n",
    "df2_test[['discover_v', 'discover_d', 'discover_t']] = \\\n",
    "    df2_test[['discover_v', 'discover_d', 'discover_t']].sample(frac=1).values\n",
    "\n",
    "df1_test = df1_test.append(df2_test).sample(frac=1).reset_index(0, drop=True)\n",
    "\n",
    "df1_test.to_csv('output_test.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tokenizer for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "input_ids = []\n",
    "token_type_ids = []\n",
    "attention_mask = []\n",
    "tokens = []\n",
    "segment_ids = []\n",
    "\n",
    "## Examples: Dict in the form {'seq_wind', 'seq_discover', 'label'}\n",
    "# 'seq_wind': [(first_triplet), (second_triplet), (third_triplet)]\n",
    "# 'seq_discover': [(first_triplet), (second_triplet), (third_triplet)]\n",
    "# 'label': [(0,0,0), (1,1,1), (0,0,0)]\n",
    "\n",
    "def custom_tokenizer(examples):\n",
    "\n",
    "    for key in examples.keys():\n",
    "\n",
    "        if key == \"label\":\n",
    "            for label in examples[key]:\n",
    "                labels.append(label)\n",
    "\n",
    "        elif key == \"seq_wind\":\n",
    "            tokens.append(1001)  ## CLS = 1001\n",
    "            segment_ids.append(0)\n",
    "            for triplet in examples[key]:\n",
    "                for el in triplet:\n",
    "                    tokens.append(int(el))\n",
    "                    segment_ids.append(0)\n",
    "            tokens.append(1002)  ## SEP = 1002\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        elif key == \"seq_discover\":\n",
    "            for triplet in examples[key]:\n",
    "                for el in triplet:\n",
    "                    tokens.append(int(el))\n",
    "                    segment_ids.append(1)\n",
    "            tokens.append(1002) ## SEP = 1002\n",
    "            segment_ids.append(1)\n",
    "\n",
    "    input_mask = [1] * len(tokens)\n",
    "\n",
    "    while len(tokens) < max_seq_length:\n",
    "        tokens.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    input_ids.append(tokens)\n",
    "    token_type_ids.append(segment_ids)\n",
    "    attention_mask.append(input_mask)\n",
    "\n",
    "    items = {'input_ids': torch.tensor(input_ids, dtype=torch.int64)}\n",
    "    items['token_type_ids'] = torch.tensor(token_type_ids, dtype=torch.int64)\n",
    "    items['attention_mask'] = torch.tensor(attention_mask, dtype=torch.float)\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = {}\n",
    "tr_dataset['seq_wind'] = list(df1_train[['wind_v','wind_d','wind_t']].values.tolist())\n",
    "tr_dataset['seq_discover'] = list(df1_train[['discover_v','discover_d','discover_t']].values.tolist())\n",
    "tr_dataset['label'] = list(df1_train[['label']].values.tolist())\n",
    "\n",
    "train_inputs = custom_tokenizer(tr_dataset)\n",
    "train_inputs['next_sentence_label'] = torch.LongTensor([labels]).T.squeeze()\n",
    "train_inputs['labels'] = train_inputs['input_ids'].detach().clone()\n",
    "\n",
    "\n",
    "## Masking tokens in the input_ids tensor using the 15% probability for MLM\n",
    "rand = torch.rand(train_inputs['input_ids'].shape)\n",
    "mask_arr = (rand < 0.15) * (train_inputs['input_ids'] != 1001) * \\\n",
    "           (train_inputs['input_ids'] != 1002) * (train_inputs['input_ids'] != 0)\n",
    "\n",
    "selection = []\n",
    "for i in range(train_inputs['input_ids'].shape[0]):\n",
    "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n",
    "\n",
    "for i in range(train_inputs['input_ids'].shape[0]):\n",
    "    train_inputs['input_ids'][i, selection[i]] = 1003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tokenizer for Set Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "input_ids = []\n",
    "token_type_ids = []\n",
    "attention_mask = []\n",
    "tokens = []\n",
    "segment_ids = []\n",
    "\n",
    "## Examples: Dict in the form {'seq_wind', 'seq_discover', 'label'}\n",
    "# 'seq_wind': [(first_triplet), (second_triplet), (third_triplet)]\n",
    "# 'seq_discover': [(first_triplet), (second_triplet), (third_triplet)]\n",
    "# 'label': [(0,0,0), (1,1,1), (0,0,0)]\n",
    "\n",
    "def custom_tokenizer(examples):\n",
    "\n",
    "    for key in examples.keys():\n",
    "\n",
    "        if key == \"label\":\n",
    "            for label in examples[key]:\n",
    "                labels.append(label)\n",
    "\n",
    "        elif key == \"seq_wind\":\n",
    "            tokens.append(1001)  ## CLS = 1001\n",
    "            segment_ids.append(0)\n",
    "            for triplet in examples[key]:\n",
    "                for el in triplet:\n",
    "                    tokens.append(int(el))\n",
    "                    segment_ids.append(0)\n",
    "            tokens.append(1002)  ## SEP = 1002\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        elif key == \"seq_discover\":\n",
    "            for triplet in examples[key]:\n",
    "                for el in triplet:\n",
    "                    tokens.append(int(el))\n",
    "                    segment_ids.append(1)\n",
    "            tokens.append(1002) ## SEP = 1002\n",
    "            segment_ids.append(1)\n",
    "\n",
    "    input_mask = [1] * len(tokens)\n",
    "\n",
    "    while len(tokens) < max_seq_length:\n",
    "        tokens.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    input_ids.append(tokens)\n",
    "    token_type_ids.append(segment_ids)\n",
    "    attention_mask.append(input_mask)\n",
    "\n",
    "    items = {'input_ids': torch.tensor(input_ids, dtype=torch.int64)}\n",
    "    items['token_type_ids'] = torch.tensor(token_type_ids, dtype=torch.int64)\n",
    "    items['attention_mask'] = torch.tensor(attention_mask, dtype=torch.float)\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dataset = {}\n",
    "te_dataset['seq_wind'] = list(df1_test[['wind_v','wind_d','wind_t']].values.tolist())\n",
    "te_dataset['seq_discover'] = list(df1_test[['discover_v','discover_d','discover_t']].values.tolist())\n",
    "te_dataset['label'] = list(df1_test[['label']].values.tolist())\n",
    "\n",
    "test_inputs = custom_tokenizer(te_dataset)\n",
    "test_inputs['next_sentence_label'] = torch.LongTensor([labels]).T.squeeze()\n",
    "test_inputs['labels'] = test_inputs['input_ids'].detach().clone()\n",
    "\n",
    "\n",
    "## Masking tokens in the input_ids tensor using the 15% probability for MLM\n",
    "rand = torch.rand(test_inputs['input_ids'].shape)\n",
    "mask_arr = (rand < 0.15) * (test_inputs['input_ids'] != 1001) * \\\n",
    "           (test_inputs['input_ids'] != 1002) * (test_inputs['input_ids'] != 0)\n",
    "\n",
    "selection = []\n",
    "for i in range(test_inputs['input_ids'].shape[0]):\n",
    "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n",
    "\n",
    "for i in range(test_inputs['input_ids'].shape[0]):\n",
    "    test_inputs['input_ids'][i, selection[i]] = 1003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASADataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, encodings, batch_size=500):\n",
    "        self.counter = 0\n",
    "        self.size = int(np.floor(encodings['input_ids'].shape[1])/batch_size)\n",
    "        self.encodings = {}\n",
    "        \n",
    "        for key in encodings.keys():\n",
    "            if key != 'next_sentence_label':\n",
    "                self.encodings[key] = encodings[key][0][:int(np.floor(encodings[key].shape[1])/batch_size)*batch_size].reshape(self.size,batch_size)\n",
    "            else:\n",
    "                self.encodings[key] = encodings[key]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NASADataset(train_inputs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
    "\n",
    "test_dataset = NASADataset(test_inputs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "epochs = 5\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loop = tqdm(train_loader, leave=True)\n",
    "    test_loop = tqdm(test_loader, leave=True)\n",
    "\n",
    "    for batch in train_loop:\n",
    "\n",
    "        train_temp_loss = 0\n",
    "        test_temp_loss = 0\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids_batch = batch['input_ids'].to(device)\n",
    "        token_type_ids_batch = batch['token_type_ids'].to(device)\n",
    "        attention_mask_batch = batch['attention_mask'].to(device)\n",
    "        next_sentence_label_batch = batch['next_sentence_label'].to(device)\n",
    "        labels_batch = batch['labels'].to(device)\n",
    "\n",
    "        # ---------- FORWARD ----------\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_mask_batch,\n",
    "                        token_type_ids=token_type_ids_batch,\n",
    "                        next_sentence_label=next_sentence_label_batch,\n",
    "                        labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # ---------- BACKWARD ----------\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # ---------- LOG ----------\n",
    "        train_loop.set_description(f'Epoch {epoch+1}')\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_temp_loss += loss.item()\n",
    "\n",
    "\n",
    "    ## Evaluate on Test Set\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in test_loop:\n",
    "\n",
    "            input_ids_batch = batch['input_ids'].to(device)\n",
    "            token_type_ids_batch = batch['token_type_ids'].to(device)\n",
    "            attention_mask_batch = batch['attention_mask'].to(device)\n",
    "            next_sentence_label_batch = batch['next_sentence_label'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids_batch, attention_mask=attention_mask_batch,\n",
    "                            token_type_ids=token_type_ids_batch,\n",
    "                            next_sentence_label=next_sentence_label_batch,\n",
    "                            labels=labels_batch)\n",
    "            test_inst_loss = outputs.loss\n",
    "\n",
    "            # ---------- LOG ----------\n",
    "            test_loop.set_description(f'Epoch {epoch+1}')\n",
    "            test_loop.set_postfix(loss=test_inst_loss.item())\n",
    "            test_temp_loss += test_inst_loss.item()\n",
    "\n",
    "    train_loss.append(train_temp_loss)\n",
    "    test_loss.append(test_temp_loss)\n",
    "\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Train Loss', color='blue')\n",
    "plt.plot(test_loss, label='Test Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('speech': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "50f798c039f92e39594af06ec0119751541d975fa6ec3b2f5528645cd2e370ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
